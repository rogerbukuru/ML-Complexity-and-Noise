---
title: "Assignment I - Complexity And Noise"
author: "Roger Bukuru"
date: "`r Sys.Date()`"
output: html_document
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1

## (a)

```{r}
# Legendre polynomials
rm(list = ls())
Legendre=function(x,q){
	val=0
	for(i in 0:q){
		val=val+((x^i)*choose(q,i)*choose((q+i-1)/2,q))
	}
	return((2^q)*val)
}

x<-seq(-1,1,0.01)
plot(x,Legendre(x,1),type="l",ylab=expression(L[n](x)),col="blue")
lines(x,Legendre(x,0),type="l",col="black")
lines(x,Legendre(x,2),type="l",col="green")
lines(x,Legendre(x,3),type="l",col="red")
lines(x,Legendre(x,4),type="l",col="orange")
lines(x,Legendre(x,5),type="l",col="purple")
```

The behaviour that we observe is that as we increase $q$ the complexity of the resulting function increases, it ranges from a function that is at the intercept to a 5-degree polynomial.

## (b)

```{r}
set.seed(2023)
# creates random polynomial target functions
rPolyfunc=function(x,q){ # Equation 2
  val=0
  alpha=runif(q,-1,1)
  for(i in 1:q){
    val=val+(x^i*alpha[i])
  }
  return(val)
}

# Equation 2 randomly generated functions
x<-seq(-1,1,0.01)
plot(x,1.5*x,type="n",ylab=expression(f(x)), main= "Equation 2 random target functions")
lines(x,rPolyfunc(x,2),type="l")
lines(x,rPolyfunc(x,3),col="blue")
lines(x,rPolyfunc(x,4),col="red")
#lines(x,rPolyfunc(x,5),col="green")

```


```{r}
# Equation 3 randomly generated functions
set.seed(2023)
rLegfunc=function(x,q){ # Equation 3
	val=0
	beta=runif(q,-1,1)
	for(i in 1:q){
		val=val+(Legendre(x,i)*beta[i])
	}
	return(val)
}

x<-seq(-1,1,0.01)
plot(x,1.1*x,type="n",ylab=expression(f(x)), main= "Equation 3 random target functions")
lines(x,rLegfunc(x,2),type="l")
lines(x,rLegfunc(x,3),col="blue")
lines(x,rLegfunc(x,4),col="red")

```

Reviewing the above two figures, we observe that for the target functions generated from equation 2, they have fewer degrees of freedom compared to the target functions generated from equation 3. As a result, they depict target functions with less model complexity compared to those from equation 3. The target functions from equation 3 are also smoother and can in theory better capture non-linear relationships, however if we push the $q$ value we may end up with more complex functions similar to those from question 1.

# Question 2

```{r}
set.seed(2023)
library(fields)
#simulates a dataset with target "func" of size n with noise sig
generator=function(n,x,func,sig){
	l<-length(func)
	dat<-matrix(rep(NA,2*n),ncol=n)
	xdat<-floor(runif(n)*l)+1
	ydat<-func[xdat]+rnorm(n,0,sig)
	xdat<-x[xdat]
	Data<-data.frame(xdat,ydat)
	return(Data)
}


#fitted model of the data
fit=function(x,model){
	v=0
	for(i in 1:length(model$coefficient)){
		v=v+(model$coefficient[i]*(x^(i-1)))
	}
return(v)
}

# gives the bias for a given fitted model (DOES NOT ACCOUNT FOR VARIABILITY IN THE DATA)
fdiff=function(x,target,model){
	f=fit(x,model)
	return((t(f-target)%*%(f-target))*(x[2]-x[1]))
}



x<-seq(-1,1,0.01);n=20;sig=0.5;q=10;
plot(c(min(x),max(x)),c(-4,4),main="10-th order target + noise",type="n",xlab="x",ylab=expression(f(x)))
func=rLegfunc(x,t)
lines(x,func,type="l",lwd=2)
d<-generator(n,x,func,sig)
points(d$xdat,d$ydat)


model_H2  = lm(d$ydat~d$xdat+I(d$xdat^2))
model_H10 = lm(d$ydat~d$xdat+I(d$xdat^2)+I(d$xdat^3)+I(d$xdat^4)+I(d$xdat^5)+I(d$xdat^6)+I(d$xdat^7)+I(d$xdat^8)+I(d$xdat^9)+I(d$xdat^10))


err.q<-(t(model_H2$residuals)%*%model_H2$residuals)/n # In sample error for the quadratic model
err.t<-(t(model_H10$residuals)%*%model_H10$residuals)/n # In sample error for the quadratic model
lines(x,fit(x,model_H2),type="l",lty=2,col="blue")
lines(x,fit(x,model_H10),type="l",lty=2,col="red")
text1=bquote(italic(E)["in"](x^2)==.(format(err.q,digits=3)))
text2=bquote(italic(E)["in"](x^10)==.(format(err.t,digits=3)))
text(x=-0.5,y=3.7,labels=text1)
text(x=-0.5,y=3.2,labels=text2)
err.out.q<-fdiff(x,func,model_H2)+(sig^2)
err.out.t<-fdiff(x,func,model_H10)+(sig^2)
text3=bquote(italic(E)["out"](x^2)==.(format(err.out.q,digits=2)))
text4=bquote(italic(E)["out"](x^{10})==.(format(err.out.t,digits=2)))
text(x=0.,y=3.7,labels=text3)
text(x=0.,y=3.2,labels=text4)


# Define ranges for N and sigma
N_vals = seq(20, 110, by = 1)
sigma_vals = seq(0.2, 1.1, by = 0.01)

# Store results
results = matrix(NA, nrow = length(N_vals), ncol = length(sigma_vals))

# Run simulations over N and sigma
for (i in 1:length(N_vals)) {
  for (j in 1:length(sigma_vals)) {
    n = N_vals[i]
    sigma = sigma_vals[j]
    
    # Generate data
    d = generator(n, x, func, sigma)
    
    # Fit models
    model_H2  = lm(d$ydat~d$xdat+I(d$xdat^2))
    model_H10 = lm(d$ydat~d$xdat+I(d$xdat^2)+I(d$xdat^3)+I(d$xdat^4)+I(d$xdat^5)+I(d$xdat^6)+I(d$xdat^7)+I(d$xdat^8)+I(d$xdat^9)+I(d$xdat^10))
    
    # Calculate out-of-sample errors
    Eout_H2 = fdiff(x,func,model_H2)+(sig^2)
    Eout_H10 = fdiff(x,func,model_H10)+(sig^2)

    # Store relative performance
    results[i, j] = Eout_H10 - Eout_H2
  }
}

# Create a color map of the relative performance
image.plot(N_vals, sigma_vals, results, xlab = "N", ylab = "sigma", main = "Relative Performance (H10 - H2)")



```


```{r}
# Define ranges for N and Q_f
N_vals = seq(20, 60, by = 1)
Qf_vals = seq(1, 40, by = 1)

# Fixed noise level
sigma = 0.2

# Store results
results = matrix(NA, nrow = length(N_vals), ncol = length(Qf_vals))

# Run simulations over N and Q_f
for (i in 1:length(N_vals)) {
  for (j in 1:length(Qf_vals)) {
    n = N_vals[i]
    qf = Qf_vals[j]
    
    # Generate the target function of order Qf
    func = rLegfunc(x, qf)
    
    # Generate data
    d = generator(n, x, func, sigma)
    
    # Fit models (H2 and H10)
    model_H2  = lm(d$ydat~d$xdat+I(d$xdat^2))
    model_H10 = lm(d$ydat~d$xdat+I(d$xdat^2)+I(d$xdat^3)+I(d$xdat^4)+I(d$xdat^5)+I(d$xdat^6)+I(d$xdat^7)+I(d$xdat^8)+I(d$xdat^9)+I(d$xdat^10))
    
    # Calculate out-of-sample errors
    Eout_H2 = fdiff(x, func, model_H2) + (sigma^2)
    Eout_H10 = fdiff(x, func, model_H10) + (sigma^2)
    
    # Store relative performance (overfit measure)
    results[i, j] = Eout_H10 - Eout_H2
  }
}

image.plot(N_vals, Qf_vals, results, xlab = "N", ylab = "Qf", main = "Relative Performance (H10 - H2)")

```


# Question 3

## (a)

```{r}

rm(list=ls())

set.seed(2023)
# Set up a target function/pattern:
f = function(x) {
  ifelse(x < rep(0, length(x)),  (abs(x + 1) - 0.5), (abs(x - 1) - 0.5))
}


# Data Generating Process
dgp = function(N, f, sig2=0)
{
   x = runif(N,-2,2)
   e = rnorm(N,0,sqrt(sig2))
   y = f(x) + e
   return(list(y = y, x = x))
}


# Hypothesis models
h1 = function(x, w0, w1) {
  w0 + w1 * x
}

h2 = function(x, w1, w2) {
  w1 * sin(pi * x) + w2 * cos(pi * x)
}

# Plot f(x) over the input space
x_vals = seq(-2, 2, length.out = 100)
plot(x_vals, f(x_vals), type = "l", col = "black", ylim = c(-2, 2), lwd = 2,
     main = "Target Function f(x) and Hypothesis Models", xlab = "x", ylab = "y")

for( i in 1:5){
  sample_data = dgp(5, f)
  h1_fit      = lm(y~x, data=sample_data)
  h1_pred     = predict(h1_fit, newdata = data.frame(x = x_vals))
  
  sample_data$z1 = sin(pi * sample_data$x)
  sample_data$z2 = cos(pi * sample_data$x)
  h2_fit = lm(y ~ z1 + z2 - 1, data = sample_data)
  h2_pred = predict(h2_fit, newdata = data.frame(z1 = sin(pi * x_vals), z2 = cos(pi * x_vals)))
  # Plot the fitted models
  lines(x_vals, h1_pred, col = "blue", lty = 2)  # h1 hypothesis
  lines(x_vals, h2_pred, col = "red", lty = 3) # h2 hypothesis
    
  # Add points for each sample
  points(sample_data$x, sample_data$y, col = "orange", pch = 19)
} 

legend("topright", legend = c("Target Function f(x)", "Hypothesis h1", "Hypothesis h2", "Sample Points"),
       col = c("black", "blue", "red", "orange"), lty = c(1, 2, 3, NA), pch = c(NA, NA, NA, 19), 
       bty = "n", cex = 0.8)
 
  
```


# (b)

Based on the plots in (a) what we observe is that hypothesis $h2$ results in the best approximation of the target function, however this doesn't mean it will be the best suited to modelling the the given data engineering process. Therefore we apriori expect it to fail to generalize out-of-sample as it is overly fitting onto the target function. The model complexity indicates that is has low-bias however when attempting to learn i.e generalize we apriori expect it to product sub-optimal results as a result we apriori believe that model $h1$ would be the more suited to modelling the given data generating process.

# (c)

```{r}
bias_var = function(N = 5, sig2 = 0, M = 1000, hypothesis = "h1", dx = 1/100)
{
	x_lat = seq(-2,2,dx)
	N_dx  = length(x_lat)
	g_bar = rep(0,N_dx)
	G_d   = matrix(0,M,N_dx)
	
	errors = rep(0,M)
	for(i in 1:M)
	{
		res_data = dgp(N,f,sig2)
		g_D = NULL
		if(hypothesis == "h1"){
		   res_model = lm(y~x, data=res_data)
		   g_D       = predict(res_model,data.frame(x = x_lat))
		   g_bar     = g_bar + g_D # Calculating the sum part in the averaging
		   G_d[i,]   = g_D # Particular realization of the data
		   dat_OOS   = dgp(N,f,sig2)
		   yhat_OOS  = predict(res_model,data.frame(x=dat_OOS$x))
		   errors[i] = mean((yhat_OOS-dat_OOS$y)^2) # Mean-squared error 
		}
	  if(hypothesis == "h2"){
		   res_data$z1  = sin(pi * res_data$x)
       res_data$z2  = cos(pi * res_data$x)
       res_model    = lm(y ~ z1 + z2 - 1, data = res_data)
       g_D          = predict(res_model, newdata = data.frame(z1 = sin(pi * x_lat), z2 = cos(pi * x_lat)))
       g_bar        = g_bar + g_D # Calculating the sum part in the averaging
       G_d[i,]      = g_D
		   dat_OOS      = dgp(N,f,sig2)
		   dat_OOS$z1   = sin(pi * dat_OOS$x)
       dat_OOS$z2   = cos(pi * dat_OOS$x)
		   yhat_OOS     =  predict(res_model, newdata = data.frame(z1 = sin(pi * dat_OOS$x), z2 = cos(pi * dat_OOS$x))) 
		   errors[i]    = mean((yhat_OOS-dat_OOS$y)^2)
	  }
	

	}
	g_bar = g_bar/M # Finally calculate the average

	test_error = mean(errors)
	phi_X = 1/2
	bias2 = sum((g_bar-f(x_lat))[-N_dx]^2*phi_X*dx) # Is the -N_dx due to the left integral ?
	
	ones = matrix(1,M,1)
	var_at_x = colSums((G_d-ones%*%g_bar)^2)/M # Variance at x across all data realisations
	var      = sum(var_at_x[-N_dx]*phi_X*dx)
	test_error_2 = bias2 + var + sig2
	
	return(list(bias2 = bias2,var = var, both = bias2+var, test_error = test_error, test_error_2 = test_error_2, g_bar = g_bar))	
}
set.seed(2023)

h1_model_bias_variance = bias_var(N = 5, sig2 = 0, M = 1000, hypothesis = "h1", dx = 1/100)
h2_model_bias_variance = bias_var(N = 5, sig2 = 0, M = 1000, hypothesis = "h2", dx = 1/100)



results <- data.frame(
  Component = c("Bias", "Variance"),
  `H_0` = c(round(h1_model_bias_variance$bias2, 4), round(h1_model_bias_variance$var, 4)),
  `H_1` = c(round(h2_model_bias_variance$bias2, 4), round(h2_model_bias_variance$var, 4))
)

#kable(results, format = "html", caption = "Bias and Variance for H0 and H1") %>%
 # kable_styling(full_width = FALSE, position = "center")

```

\begin{table}[h!]
\centering
\caption{Bias and Variance for H0 and H1}
\begin{tabular}{lcc}
\toprule
Component & $H_0$ & $H_1$ \\
\midrule
Bias     & 0.1675 & 0.0024 \\
Variance & 0.1728 & 0.0016 \\
\bottomrule
\end{tabular}
\end{table}


```{r}
set.seed(2023)
sigma_values = c(0, 0.1, 0.2, 0.3, 0.4, 0.5)
bias_values  = data.frame( h1 = rep(0, length(sigma_values)), h2 = rep(0, length(sigma_values)) )
variance_values  = data.frame( h1 =rep(0, length(sigma_values)), h2= rep(0, length(sigma_values)) )
expected_oss_1_values  = data.frame( h1 =rep(0, length(sigma_values)), h2= rep(0, length(sigma_values)) )
expected_oss_2_values  = data.frame( h1 =rep(0, length(sigma_values)), h2= rep(0, length(sigma_values)) )


for ( i in 1:length(sigma_values)){
  h1_model_bias_variance = bias_var(N = 5, sig2 = sigma_values[i], M = 1000, hypothesis = "h1", dx = 1/100)
  h2_model_bias_variance = bias_var(N = 5, sig2 = sigma_values[i], M = 1000, hypothesis = "h2", dx = 1/100)
  
  bias_values$h1[i] = h1_model_bias_variance$bias2
  bias_values$h2[i] = h2_model_bias_variance$bias2
  
  variance_values$h1[i] = h1_model_bias_variance$var
  variance_values$h2[i] = h2_model_bias_variance$var
  
  expected_oss_1_values$h1[i] = h1_model_bias_variance$test_error
  expected_oss_1_values$h2[i] = h2_model_bias_variance$test_error
  
  expected_oss_2_values$h1[i] = h1_model_bias_variance$test_error_2
  expected_oss_2_values$h2[i] = h2_model_bias_variance$test_error_2
}


# Plot 1

plot(sigma_values, bias_values$h1, type="l", xlab= "Sigma", ylab="Bias", ylim=c(0, 0.30), main="Bias for H1 and H2", col="red", lwd=2)
lines(sigma_values, bias_values$h2, col="orange", lwd=2)
legend("topleft", legend = c("Hypothesis h1", "Hypothesis h2"),
       col = c("red", "orange"), lty = c(1, 1), 
       bty = "n", cex = 0.8)
# Plot 2

plot(sigma_values, variance_values$h1, type="l", xlab= "sigma", ylab="Variance", ylim=c(0, 1), main="Variance for H1 and H2", col="red", lwd=2)
lines(sigma_values, variance_values$h2, col="orange", lwd=2)
legend("topleft", legend = c("Hypothesis h1", "Hypothesis h2"),
       col = c("red", "orange"), lty = c(1, 1), 
       bty = "n", cex = 0.8)

# Plot 3
plot(sigma_values, expected_oss_1_values$h1, type="l", xlab= "Sigma", ylab="Expected Error", ylim=c(0, 2), main="OSS Expected Error for H1 and H2", col="red", lwd=2)
lines(sigma_values, expected_oss_2_values$h1, col="orange", lwd=2)
lines(sigma_values, expected_oss_1_values$h2, col="blue", lwd=2)
lines(sigma_values, expected_oss_2_values$h2, col="purple", lwd=2)
legend("topleft", legend = c("MSE OSS-Error H1", "BV+Noise OSS-Error H1", "MSE OSS-Error MSE H2", "BV+Noise OSS-Error H2"),
       col = c("red", "orange", "blue", "purple"), lty = c(1, 1, 1,1), 
       bty = "n", cex = 0.8)
```


The bias variance dynamics above shows that as we introduce noise into the data generating process, the bias term is not affected or at least the effect is ignorable, however the variance component is significantly effect by the noise as we observe that the variance increase with an increase in sigma. With respect to the individual models we observe that the noise component had the largest affect on the simpler H1 model, the more complex model better handles the introduction of noise as its rate of increase in variance is less erratic compared to H1. The out-of-sample dyanamics further support this as we observe that H2 has better OOS performance as the noise increases. 

Finally for the given data generating process, and the given realized data set (data resources) we think that model H2 would be the most appropriate as it has the better out-of-sample performance, indicating that it generalizes better. Moreover from the results we note that it has the better out-of-sample performance for both when we have noise and in the case where we don't have noise.


# Question 4

#a

```{r}
rm(list=ls())
library(ggplot2)
library(tidyverse)

dat = read.table("Collider_Data_2022.txt", h = TRUE, stringsAsFactors = TRUE)
dim(dat)
head(dat)
X = as.matrix(dat[,1:2])
Y = as.matrix(dat[,3:5])
non_one_hot = rep(0, nrow(X))

for ( i in 1: length(non_one_hot)){
  obs = Y[i, ]
  if(obs[1] == 1){
    non_one_hot[i] = 1
  }else if (obs[2] == 1) {
    non_one_hot[i] = 2
  }else if (obs[3] == 1) {
     non_one_hot[i] = 3
  }
}


dat = dat %>% as.tibble() %>%
      mutate(class = non_one_hot)


ggplot(dat, aes(x =X1 , y = X2, color = as.factor(class))) +
  geom_point(size = 3) +
  labs(title = "Collider Observations", x = "X1", y = "X2", color="Class") +
  theme_minimal()


cols = c('red','blue', "orange")
plot(dat$X2~dat$X1,col = cols[dat$class] ,pch = 16)
```

Based on the plot of the feature space, we observe that non-linear machinery such as a neural network would be an appropriate model class, given that the data is non-linearly separable.

#b

```{r}
soft_max = function(z) {
  exp_z    = exp(z)
  col_sums = apply(exp_z, 2, sum)
  soft_max_result = sweep(exp_z,2, col_sums, "/")
  return(soft_max_result)
}


```

#c

```{r}

sig1 = function(z)
{
  tanh(z)
}


neural_net = function(X, Y, theta,m,nu) {
  
   N = dim(X)[1]
   p = dim(X)[2]
   q = dim(Y)[2]
   dims = c(p,m,q)
  
  
   index = 1:(dims[1]*dims[2])
   W1    = matrix(theta[index],dims[1],dims[2]) # weights for d1
   index = max(index)+1:(dims[2]*dims[3])
   W2    = matrix(theta[index],dims[2],dims[3]) # weights for d2
   index = max(index)+1:(dims[2])
   b1    = matrix(theta[index],dims[2],1)
   index = max(index)+1:(dims[3])
   b2    = matrix(theta[index],dims[3],1)
   
   ones = matrix(1,1,N)
   
   A0 = t(X)
   A1 = sig1(t(W1)%*%A0+b1%*%ones)
   A2 = soft_max(t(W2)%*%A1+b2%*%ones)
   

   y_hat         = t(A2)
   error         = rep(0,N)
   y_i_1         = which(apply(Y, 1, function(row) all(row == c(1, 0, 0))))
   y_i_2         = which(apply(Y, 1, function(row) all(row == c(0, 1, 0))))
   y_i_3         = which(apply(Y, 1, function(row) all(row == c(0, 0, 1))))
   error[y_i_1]  = (Y[y_i_1,]*(log(y_hat[y_i_1,])))[,1]
   error[y_i_2]  = (Y[y_i_2,]*(log(y_hat[y_i_2,])))[,2]
   error[y_i_3]  = (Y[y_i_3,]*(log(y_hat[y_i_3,])))[,3]
   
   
   E1 = -1/N*sum(error)
   E2 = E1 + nu*(sum(W1^2)+sum(W2^2))/N
   # Return a list of relevant objects:
   return(list(A2 = A2,A1 = A1, E1 = E1, E2 = E2))

}

nu    = 0.1
m     = 9

p     = dim(X)[2] 
q     = dim(Y)[2]
npars = p*m+m*q+m+q
theta_rand = runif(npars,-1,1)

res_model = neural_net(X,Y,theta_rand,m,nu)


obj_pen = function(pars)
{
  res_model = neural_net(X,Y,pars,m,nu)
  return(res_model$E2)
}

#obj_pen(theta_rand)
#res_opt = nlm(obj_pen,theta_rand, iterlim = 1000)
```
Yes there are practical pitfalls if say for example our $y_ij$ i.e the observed classification is encoded as a zero, this could lead to multiple the logs with 0, therefore resulting in an error value of 0, which is not true.

Yes there are practical pitfalls when we have predictions that are zero, in our cases the one hot-encoded will always contain zero values, resulting in issues when computing the log of that value, to account for this we can add a small epsilon value

#d

```{r}
set.seed(2022)

train_indices = sample(1:nrow(X), size=nrow(X)*0.5,replace=FALSE)
x_train       = X[train_indices,]
y_train       = Y[train_indices,]
x_val         = X[-train_indices,]
y_val         = Y[-train_indices,]

m              = 9
n_nus          = 10
nu_seq         = exp(seq(-5,0,length = n_nus))
E_val          = rep(NA,n_nus)
params         = list()


obj_pen = function(pars)
{
  res_model = neural_net(x_train,y_train,pars,m,nu)
  return(res_model$E2)
}

for(i in 1:n_nus)
{
  nu         = nu_seq[i]
  res_opt    = nlm(obj_pen,theta_rand, iterlim = 1000)

  # Extract Validation Error:
  res_val  = neural_net(x_val,y_val,res_opt$estimate,m,0)
  E_val[i] = res_val$E1
  params[[i]] = res_opt$estimate


}

plot(nu_seq, E_val, type="b", xlab="nu", ylab="validaton", main="Validation vs nu", col="red")

index_min_val  = which.min(E_val)
optimal_nu     = nu_seq[index_min_val]
optimal_params = params[[index_min_val]]

```

# e

```{r}
library(colorspace)
color.gradient = function(x, colors=c('magenta','green','lightblue'), colsteps=50)
{
  colpal = colorRampPalette(colors)
  return( colpal(colsteps)[ findInterval(x, seq(min(x),max(x), length=colsteps)) ] )
}


# Draw a response curve over the 2D input space to see
# what pattern the neural network predicts
cols = c('red','blue', "orange")
plot(dat$X2~dat$X1,col = cols[dat$class] ,pch = 16)
n  = optimal_nu
M  = 100
x1 = seq(-4,4,length = M)
x2 = seq(-4,4,length = M)
xx1 = rep(x1,M)
xx2 = rep(x2,each = M)

abline(v = x1,h= x2)
points(xx2~xx1,pch = 16, col = 'yellow',cex = 2)

XX = cbind(xx1,xx2) 
YY = matrix(c(1,0,0, 0,1,0,0,0,1),M^2,ncol=3)
res_fitted = neural_net(XX,YY,optimal_params,m,nu) # use NN to make predictions
predictions = res_fitted$A2
pred_class = apply(predictions,2, which.max)

plot(XX[,2]~XX[,1], pch = 16, col = color.gradient(pred_class),cex = 1/2, main="Response Curve", xlab="X1", ylab="X2")
points(X[,2]~X[,1], col = cols[dat$class], pch = 16)
```

This response curve can be used to classify new particles, by plotting the new particles against the response curve and see how they are seperated/classified on the input space.
